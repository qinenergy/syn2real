{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Syn2real-CWRU",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt8yLVTAdAYh",
        "outputId": "65c1b1e7-2b7f-40e1-f130-a7f164d5db2a"
      },
      "source": [
        "\n",
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "!rm -f *.npy\n",
        "\n",
        "\n",
        "!wget https://qin.ee/bearings/e90769db282a3b585b97cbb05183423d/XsynallDEenv.npy\n",
        "!wget https://qin.ee/bearings/e90769db282a3b585b97cbb05183423d/XreallDEenv.npy\n",
        "!wget https://qin.ee/bearings/e90769db282a3b585b97cbb05183423d/ysynallDEenv.npy\n",
        "!wget https://qin.ee/bearings/e90769db282a3b585b97cbb05183423d/yreallDEenv.npy\n",
        "\n",
        "!md5sum XsynallDEenv.npy\n",
        "\n",
        "# 4096 100 -> (4000, 1000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "--2021-01-20 16:21:00--  https://qin.ee/bearings/e90769db282a3b585b97cbb05183423d/XsynallDEenv.npy\n",
            "Resolving qin.ee (qin.ee)... 35.177.27.26\n",
            "Connecting to qin.ee (qin.ee)|35.177.27.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32000128 (31M) [application/octet-stream]\n",
            "Saving to: ‘XsynallDEenv.npy’\n",
            "\n",
            "XsynallDEenv.npy    100%[===================>]  30.52M  95.6MB/s    in 0.3s    \n",
            "\n",
            "2021-01-20 16:21:02 (95.6 MB/s) - ‘XsynallDEenv.npy’ saved [32000128/32000128]\n",
            "\n",
            "--2021-01-20 16:21:02--  https://qin.ee/bearings/e90769db282a3b585b97cbb05183423d/XreallDEenv.npy\n",
            "Resolving qin.ee (qin.ee)... 35.177.27.26\n",
            "Connecting to qin.ee (qin.ee)|35.177.27.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32000128 (31M) [application/octet-stream]\n",
            "Saving to: ‘XreallDEenv.npy’\n",
            "\n",
            "XreallDEenv.npy     100%[===================>]  30.52M   103MB/s    in 0.3s    \n",
            "\n",
            "2021-01-20 16:21:13 (103 MB/s) - ‘XreallDEenv.npy’ saved [32000128/32000128]\n",
            "\n",
            "--2021-01-20 16:21:13--  https://qin.ee/bearings/e90769db282a3b585b97cbb05183423d/ysynallDEenv.npy\n",
            "Resolving qin.ee (qin.ee)... 35.177.27.26\n",
            "Connecting to qin.ee (qin.ee)|35.177.27.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32128 (31K) [application/octet-stream]\n",
            "Saving to: ‘ysynallDEenv.npy’\n",
            "\n",
            "ysynallDEenv.npy    100%[===================>]  31.38K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-01-20 16:21:13 (2.84 MB/s) - ‘ysynallDEenv.npy’ saved [32128/32128]\n",
            "\n",
            "--2021-01-20 16:21:14--  https://qin.ee/bearings/e90769db282a3b585b97cbb05183423d/yreallDEenv.npy\n",
            "Resolving qin.ee (qin.ee)... 35.177.27.26\n",
            "Connecting to qin.ee (qin.ee)|35.177.27.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32128 (31K) [application/octet-stream]\n",
            "Saving to: ‘yreallDEenv.npy’\n",
            "\n",
            "yreallDEenv.npy     100%[===================>]  31.38K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-01-20 16:21:14 (2.79 MB/s) - ‘yreallDEenv.npy’ saved [32128/32128]\n",
            "\n",
            "e90769db282a3b585b97cbb05183423d  XsynallDEenv.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHSL_ow9oNoT"
      },
      "source": [
        "class FlipGradientBuilder(object):\n",
        "    def __init__(self):\n",
        "        self.num_calls = 0\n",
        "\n",
        "    def __call__(self, x, l=1.0):\n",
        "        grad_name = \"FlipGradient%d\" % self.num_calls\n",
        "        @ops.RegisterGradient(grad_name)\n",
        "        def _flip_gradients(op, grad):\n",
        "            return [tf.negative(grad) * l]\n",
        "        \n",
        "        g = tf.get_default_graph()\n",
        "        with g.gradient_override_map({\"Identity\": grad_name}):\n",
        "            y = tf.identity(x)\n",
        "            \n",
        "        self.num_calls += 1\n",
        "        return y\n",
        "    \n",
        "flip_gradient = FlipGradientBuilder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIeSguoSdEXb"
      },
      "source": [
        "def eval(pred, y):\n",
        "    y0 = pred[y==0]\n",
        "    y1 = pred[y==1]\n",
        "    y2 = pred[y==2]\n",
        "    y3 = pred[y==3]\n",
        "    acc = np.sum(y0==0)/len(y0)\n",
        "    acc += np.sum(y1==1)/len(y1)\n",
        "    acc += np.sum(y2==2)/len(y2)\n",
        "    acc += np.sum(y3==3)/len(y3)\n",
        "    print(\"per class\", np.sum(y0==0)/len(y0), np.sum(y1==1)/len(y1), np.sum(y2==2)/len(y2), np.sum(y3==3)/len(y3))\n",
        "    return acc/4\n",
        "\n",
        "\n",
        "\n",
        "def feature_extractor(x, training=True, reuse=False, scope=\"\"):\n",
        "    with tf.variable_scope(\"feature_ext\"+scope, reuse= reuse):\n",
        "        h = tf.expand_dims(x, axis=-1)\n",
        "        h = tf.layers.conv1d(h, 10, 3, activation=tf.nn.relu, name=\"f_conv1\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        h = tf.layers.dropout(h, training=training)\n",
        "        h = tf.layers.conv1d(h, 10, 3, activation=tf.nn.relu, name=\"f_conv2\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        h = tf.layers.dropout(h, training=training)\n",
        "        h = tf.layers.conv1d(h, 10, 3, activation=tf.nn.relu, name=\"f_conv3\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        h = tf.layers.dropout(h, training=training)\n",
        "        h = tf.layers.flatten(h, name=\"f_flat\")\n",
        "        h = tf.layers.dense(h, args.size, activation=tf.nn.relu, name=\"f_fc\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        h = tf.layers.dropout(h, training=training)\n",
        "    return h\n",
        "\n",
        "\n",
        "def clf(x, training=True, reuse=False, scope=\"\"):\n",
        "    with tf.variable_scope(\"clf\"+scope, reuse= reuse):\n",
        "        h = tf.layers.dense(x, args.size, activation=tf.nn.relu, name=\"clf_fc1\")\n",
        "        h = tf.layers.dense(h, 4, name=\"clf_fc\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
        "    return h\n",
        "\n",
        "\n",
        "def discriminator(x, training=True, reuse=False, scope=0):\n",
        "    with tf.variable_scope(\"discrimiator\"+str(scope), reuse= not training or reuse):\n",
        "        h = tf.layers.dense(x, args.size*2, activation=tf.nn.relu, name=\"dis_fc1\", kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        h = tf.layers.dense(h, 2, name=\"clf_fc\")\n",
        "    return h\n",
        "\n",
        "def grl(src, tgt, y, training, mode=\"uni\"):\n",
        "    if training:\n",
        "        feat_src = feature_extractor(src, training=training)\n",
        "        logits = clf(feat_src, training=training)\n",
        "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
        "        correct_prediction = tf.equal(y, tf.argmax(logits, axis=-1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        global_step = tf.Variable(0., trainable=False, name='global_step')\n",
        "        p = args.p\n",
        "        if mode == \"dann\":\n",
        "            feat_tgt = feature_extractor(tgt, training=training, reuse=True)\n",
        "            feat = tf.concat([feat_src, feat_tgt], axis=0)\n",
        "            feat = flip_gradient(feat, p)\n",
        "            logits_dm = discriminator(feat)\n",
        "            labels_dm = tf.concat([tf.ones([args.batch_size], tf.int32), tf.zeros([args.batch_size], tf.int32)], axis=0)\n",
        "            loss_dm = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_dm, logits=logits_dm))\n",
        "            loss_total = loss  + loss_dm\n",
        "        elif mode == \"cdan\":\n",
        "            feat_tgt = feature_extractor(tgt, training=training, reuse=True)\n",
        "            logits_tgt = clf(feat_tgt, training=False, reuse=True)\n",
        "            softmax_output_tgt = tf.stop_gradient(tf.nn.softmax(logits_tgt, axis=-1))\n",
        "            softmax_output_src = tf.stop_gradient(tf.nn.softmax(logits, axis=-1))\n",
        "            feat_src_ = tf.reshape(tf.matmul(tf.reshape(softmax_output_src, [-1, 4, 1]), tf.reshape(feat_src, [-1, 1, args.size])), [-1, 4*args.size])\n",
        "            feat_tgt_ = tf.reshape(tf.matmul(tf.reshape(softmax_output_tgt, [-1, 4, 1]), tf.reshape(feat_tgt, [-1, 1, args.size])), [-1, 4*args.size])\n",
        "            feat = tf.concat([feat_src_, feat_tgt_], axis=0)\n",
        "            feat = flip_gradient(feat, p)\n",
        "            logits_dm = discriminator(feat)\n",
        "            labels_dm = tf.concat([tf.ones([args.batch_size], tf.int32), tf.zeros([args.batch_size], tf.int32)], axis=0)\n",
        "            loss_dm = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_dm, logits=logits_dm))\n",
        "            loss_total = loss  + loss_dm\n",
        "        elif mode == \"cdanmixup\":\n",
        "            dist_beta = tf.distributions.Beta(args.beta, args.beta)\n",
        "            lmb = tf.reshape(dist_beta.sample(tf.shape(src)[0]), [-1, 1])\n",
        "            labels_dm = tf.concat([tf.ones([args.batch_size], tf.int32), tf.zeros([args.batch_size], tf.int32)], axis=0)\n",
        "            feat_tgt = feature_extractor(tgt, training=training, reuse=True)\n",
        "            logits_tgt = clf(feat_tgt, training=False, reuse=True)\n",
        "            softmax_output_tgt = tf.stop_gradient(tf.nn.softmax(logits_tgt, axis=-1))\n",
        "            softmax_output_src = tf.stop_gradient(tf.nn.softmax(logits, axis=-1))\n",
        "            idxx = tf.random.shuffle(tf.range(tf.shape(feat_src)[0]))\n",
        "            feat_src = lmb * feat_src + (1.-lmb) * tf.gather(feat_src, idxx)\n",
        "            feat_tgt = lmb * feat_tgt + (1.-lmb) * tf.gather(feat_tgt, idxx)\n",
        "            softmax_output_src = lmb * softmax_output_src + (1.-lmb) * tf.gather(softmax_output_src, idxx)\n",
        "            softmax_output_tgt = lmb * softmax_output_tgt + (1.-lmb) * tf.gather(softmax_output_tgt, idxx)\n",
        "            feat_src_ = tf.reshape(tf.matmul(tf.reshape(softmax_output_src, [-1, 4, 1]), tf.reshape(feat_src, [-1, 1, args.size])), [-1, 4*args.size])\n",
        "            feat_tgt_ = tf.reshape(tf.matmul(tf.reshape(softmax_output_tgt, [-1, 4, 1]), tf.reshape(feat_tgt, [-1, 1, args.size])), [-1, 4*args.size])\n",
        "            feat = tf.concat([feat_src_, feat_tgt_], axis=0)\n",
        "            feat = flip_gradient(feat, p)\n",
        "            logits_dm = discriminator(feat)\n",
        "            loss_dm = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_dm, logits=logits_dm))\n",
        "            loss_total = loss  + loss_dm\n",
        "        else:\n",
        "            loss_total = loss \n",
        "\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate=args.lr).minimize(loss_total, global_step=global_step)\n",
        "        return loss_total, train_op\n",
        "    else:\n",
        "        feat_src = feature_extractor(src, training=False, reuse=True)\n",
        "        logits = clf(feat_src, training=False, reuse=True)\n",
        "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
        "        pred = tf.argmax(logits, axis=-1)\n",
        "        correct_prediction = tf.equal(y, pred)\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        return loss, accuracy, pred, y\n",
        "        \n",
        "\n",
        "def run():\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.set_random_seed(args.seed)\n",
        "    seed(args.seed)\n",
        "    data_src = data_src_fft.astype(np.float32)\n",
        "    data_tgt = data_tgt_fft.astype(np.float32)\n",
        "    data_tgt_up = data_tgt_fft_up.astype(np.float32)\n",
        "    \n",
        "    args.length = len(data_src)\n",
        "\n",
        "    dataset_src = tf.data.Dataset.from_tensor_slices((data_src, label_src)).repeat(args.num_epoch).shuffle(len(label_src)).batch(args.batch_size).prefetch(10)\n",
        "    iterator_src = dataset_src.make_initializable_iterator()\n",
        "    src_x, src_y = iterator_src.get_next()\n",
        "\n",
        "    data_val, label_val = data_tgt[:], label_tgt[:]\n",
        "    \n",
        "    dataset_tgt = tf.data.Dataset.from_tensor_slices((data_tgt_up, label_tgt_up)).repeat(args.num_epoch * (len(data_src) // len(data_tgt_up) + 1)).shuffle(len(label_tgt_up)).batch(args.batch_size).prefetch(10)\n",
        "    dataset_val = tf.data.Dataset.from_tensors((data_val, label_val))\n",
        "    iterator_tgt = dataset_tgt.make_initializable_iterator()\n",
        "    iterator_val = dataset_val.make_initializable_iterator()\n",
        "\n",
        "    train_init_op = [iterator_src.initializer, iterator_tgt.initializer]\n",
        "    test_init_op = iterator_val.initializer\n",
        "    tgt_x, tgt_y = iterator_tgt.get_next()\n",
        "    test_x, test_y = iterator_val.get_next()\n",
        "    \n",
        "    train_loss, train_op = grl(src_x, tgt_x, src_y, training=True, mode=args.model)\n",
        "    test_loss, acc_tgt, pred, ground_truth = grl(test_x, test_x, test_y, training=False)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run([train_init_op])\n",
        "        for ep in range(args.num_epoch):\n",
        "            for j in range(len(data_src)//args.batch_size):\n",
        "                train_loss_, _= sess.run([train_loss, train_op])\n",
        "            sess.run([test_init_op])\n",
        "            test_loss_, acc_, pred_, ground_truth_ = sess.run([test_loss, acc_tgt, pred, ground_truth])\n",
        "        print(\"Test Accuracy\" ,acc_)\n",
        "        balanced_acc = eval(pred_, ground_truth_)\n",
        "        print(\"Test Balanced Accuracy\", balanced_acc)\n",
        "    return balanced_acc, acc_\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA5tFB08gwdW"
      },
      "source": [
        "## Source only baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlTROHyOg29X",
        "outputId": "e0ba04c2-11a9-456c-97fd-bb1808a70a49"
      },
      "source": [
        "data_src_fft_ori = np.load(\"XsynallDEenv.npy\")\n",
        "label_src_ori = np.load(\"ysynallDEenv.npy\")\n",
        "data_tgt_fft_ori = np.load(\"XreallDEenv.npy\")\n",
        "label_tgt_ori = np.load(\"yreallDEenv.npy\")\n",
        "# group inner race/outer race/ball faults together\n",
        "label_src_ori = np.ceil(label_src_ori/3.).astype(np.int64) \n",
        "label_tgt_ori = np.ceil(label_tgt_ori/3.).astype(np.int64)\n",
        "\n",
        "# source data\n",
        "idx0 = np.where(label_src_ori==0)\n",
        "idx1 = np.where(label_src_ori==1)\n",
        "idx2 = np.where(label_src_ori==2)\n",
        "idx3 = np.where(label_src_ori==3)\n",
        "\n",
        "# avoid healthy sample overlap\n",
        "seed(0)\n",
        "idx_healthy = idx0[0].copy()\n",
        "np.random.shuffle(idx_healthy)\n",
        "num_healthy = len(idx_healthy)\n",
        "idx_src_healthy = np.concatenate([idx_healthy[:num_healthy//2]]*6, axis=0)\n",
        "idx_tgt_healthy = np.concatenate([idx_healthy[num_healthy//2:]]*6, axis=0)\n",
        "# balance source classes\n",
        "idx = np.concatenate([idx_src_healthy, idx1[0], idx2[0], idx3[0]], axis=0)\n",
        "label_src = label_src_ori[idx]\n",
        "data_src_fft = data_src_fft_ori[idx]\n",
        "\n",
        "# Target test data, avoid healthy sample overlap\n",
        "data_tgt_fft = np.concatenate([data_tgt_fft_ori[label_tgt_ori!=0], data_src_fft_ori[idx_tgt_healthy]], axis=0)\n",
        "label_tgt = np.concatenate([label_tgt_ori[label_tgt_ori!=0], label_src_ori[idx_tgt_healthy]], axis=0)\n",
        "\n",
        "# Target training data not used \n",
        "label_tgt_up = np.zeros_like(label_tgt.copy())\n",
        "data_tgt_fft_up = data_tgt_fft.copy()\n",
        "\n",
        "print(\"source\", \"0\", np.sum(label_src==0),\"1\",np.sum(label_src==1), \"2\",np.sum(label_src==2), \"3\", np.sum(label_src==3),)\n",
        "print(\"target train\", \"0\", np.sum(label_tgt_up==0),\"1\",np.sum(label_tgt_up==1), \"2\",np.sum(label_tgt_up==2), \"3\", np.sum(label_tgt_up==3),)\n",
        "print(\"target test\", \"0\", np.sum(label_tgt==0),\"1\",np.sum(label_tgt==1), \"2\",np.sum(label_tgt==2), \"3\", np.sum(label_tgt==3),)\n",
        "\n",
        "class args:\n",
        "    batch_size=128\n",
        "    size=256\n",
        "    num_epoch=100\n",
        "    lr=0.001\n",
        "    model=\"baseline\"\n",
        "    p=0.1 # meta info not used by baseline\n",
        "\n",
        "res = []\n",
        "res_im = []\n",
        "\n",
        "for i in range(10): \n",
        "    args.seed=i\n",
        "    acc_b, acc_i = run()\n",
        "    res.append(acc_b)\n",
        "    res_im.append(acc_i)\n",
        "\n",
        "print(\"balanced acuracy\", np.mean(res), \"+-\" , np.std(res))\n",
        "print(\"acuracy\",np.mean(res_im), \"+-\" , np.std(res_im))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source 0 1200 1 1200 2 1200 3 1200\n",
            "target train 0 4800 1 0 2 0 3 0\n",
            "target test 0 1200 1 1200 2 1200 3 1200\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-3-466148e62d1e>:119: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-466148e62d1e>:18: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv1D` instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/convolutional.py:218: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-466148e62d1e>:19: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-466148e62d1e>:24: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-466148e62d1e>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "Test Accuracy 0.5858333\n",
            "per class 1.0 1.0 0.01 0.3333333333333333\n",
            "Test Balanced Accuracy 0.5858333333333333\n",
            "Test Accuracy 0.5883333\n",
            "per class 1.0 1.0 0.02 0.3333333333333333\n",
            "Test Balanced Accuracy 0.5883333333333334\n",
            "Test Accuracy 0.61833334\n",
            "per class 1.0 1.0 0.14 0.3333333333333333\n",
            "Test Balanced Accuracy 0.6183333333333334\n",
            "Test Accuracy 0.58979166\n",
            "per class 1.0 1.0 0.025833333333333333 0.3333333333333333\n",
            "Test Balanced Accuracy 0.5897916666666667\n",
            "Test Accuracy 0.59229165\n",
            "per class 1.0 1.0 0.035833333333333335 0.3333333333333333\n",
            "Test Balanced Accuracy 0.5922916666666667\n",
            "Test Accuracy 0.5860417\n",
            "per class 1.0 1.0 0.010833333333333334 0.3333333333333333\n",
            "Test Balanced Accuracy 0.5860416666666667\n",
            "Test Accuracy 0.60833335\n",
            "per class 1.0 1.0 0.1 0.3333333333333333\n",
            "Test Balanced Accuracy 0.6083333333333334\n",
            "Test Accuracy 0.5985417\n",
            "per class 1.0 1.0 0.060833333333333336 0.3333333333333333\n",
            "Test Balanced Accuracy 0.5985416666666667\n",
            "Test Accuracy 0.58729166\n",
            "per class 1.0 1.0 0.015833333333333335 0.3333333333333333\n",
            "Test Balanced Accuracy 0.5872916666666667\n",
            "Test Accuracy 0.60020834\n",
            "per class 1.0 1.0 0.0675 0.3333333333333333\n",
            "Test Balanced Accuracy 0.6002083333333333\n",
            "balanced acuracy 0.5955 +- 0.010305321575660715\n",
            "acuracy 0.5955 +- 0.010305329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eghdy8GsloIq"
      },
      "source": [
        "## Balanced DANN - shows the upper-limit for DANN when all classes are balanced\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aziIAABblnNa",
        "outputId": "c5f899c4-d32c-417a-ed5a-869d585dc51f"
      },
      "source": [
        "data_src_fft_ori = np.load(\"XsynallDEenv.npy\")\n",
        "label_src_ori = np.load(\"ysynallDEenv.npy\")\n",
        "data_tgt_fft_ori = np.load(\"XreallDEenv.npy\")\n",
        "label_tgt_ori = np.load(\"yreallDEenv.npy\")\n",
        "# group inner race/outer race/ball faults together\n",
        "label_src_ori = np.ceil(label_src_ori/3.).astype(np.int64) \n",
        "label_tgt_ori = np.ceil(label_tgt_ori/3.).astype(np.int64)\n",
        "\n",
        "# source data\n",
        "idx0 = np.where(label_src_ori==0)\n",
        "idx1 = np.where(label_src_ori==1)\n",
        "idx2 = np.where(label_src_ori==2)\n",
        "idx3 = np.where(label_src_ori==3)\n",
        "\n",
        "# avoid healthy sample overlap\n",
        "seed(0)\n",
        "idx_healthy = idx0[0].copy()\n",
        "np.random.shuffle(idx_healthy)\n",
        "num_healthy = len(idx_healthy)\n",
        "idx_src_healthy = np.concatenate([idx_healthy[:num_healthy//2]]*6, axis=0)\n",
        "idx_tgt_healthy = np.concatenate([idx_healthy[num_healthy//2:]]*6, axis=0)\n",
        "# balance source classes\n",
        "idx = np.concatenate([idx_src_healthy, idx1[0], idx2[0], idx3[0]], axis=0)\n",
        "label_src = label_src_ori[idx]\n",
        "data_src_fft = data_src_fft_ori[idx]\n",
        "\n",
        "# Target test data, avoid healthy sample overlap\n",
        "data_tgt_fft = np.concatenate([data_tgt_fft_ori[label_tgt_ori!=0], data_src_fft_ori[idx_tgt_healthy]], axis=0)\n",
        "label_tgt = np.concatenate([label_tgt_ori[label_tgt_ori!=0], label_src_ori[idx_tgt_healthy]], axis=0)\n",
        "\n",
        "# Target training data not used \n",
        "label_tgt_up = label_tgt.copy()\n",
        "data_tgt_fft_up = data_tgt_fft.copy()\n",
        "\n",
        "print(\"source\", \"0\", np.sum(label_src==0),\"1\",np.sum(label_src==1), \"2\",np.sum(label_src==2), \"3\", np.sum(label_src==3),)\n",
        "print(\"target train\", \"0\", np.sum(label_tgt_up==0),\"1\",np.sum(label_tgt_up==1), \"2\",np.sum(label_tgt_up==2), \"3\", np.sum(label_tgt_up==3),)\n",
        "print(\"target test\", \"0\", np.sum(label_tgt==0),\"1\",np.sum(label_tgt==1), \"2\",np.sum(label_tgt==2), \"3\", np.sum(label_tgt==3),)\n",
        "\n",
        "label_tgt_up = np.zeros_like(label_tgt_up)\n",
        "\n",
        "res = []\n",
        "res_im = []\n",
        "class args:\n",
        "    batch_size=128\n",
        "    size=256\n",
        "    num_epoch=100\n",
        "    lr=0.001\n",
        "    model=\"dann\"\n",
        "    p=0.1\n",
        "\n",
        "for i in range(10): \n",
        "    args.seed=i \n",
        "    acc_b, acc_i = run()\n",
        "    res.append(acc_b)\n",
        "    res_im.append(acc_i)\n",
        "print(\"balanced acuracy\", np.mean(res), \"+-\" , np.std(res))\n",
        "print(\"acuracy\", np.mean(res_im), \"+-\" , np.std(res_im))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source 0 1200 1 1200 2 1200 3 1200\n",
            "target train 0 1200 1 1200 2 1200 3 1200\n",
            "target test 0 1200 1 1200 2 1200 3 1200\n",
            "Test Accuracy 0.84270835\n",
            "per class 1.0 1.0 0.7041666666666667 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8427083333333333\n",
            "Test Accuracy 0.83229166\n",
            "per class 0.98 1.0 0.6825 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8322916666666667\n",
            "Test Accuracy 0.79833335\n",
            "per class 1.0 1.0 0.5266666666666666 0.6666666666666666\n",
            "Test Balanced Accuracy 0.7983333333333332\n",
            "Test Accuracy 0.7914583\n",
            "per class 1.0 1.0 0.49833333333333335 0.6675\n",
            "Test Balanced Accuracy 0.7914583333333334\n",
            "Test Accuracy 0.78541666\n",
            "per class 1.0 1.0 0.475 0.6666666666666666\n",
            "Test Balanced Accuracy 0.7854166666666667\n",
            "Test Accuracy 0.83229166\n",
            "per class 1.0 1.0 0.6625 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8322916666666667\n",
            "Test Accuracy 0.84583336\n",
            "per class 1.0 1.0 0.7166666666666667 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8458333333333333\n",
            "Test Accuracy 0.7972917\n",
            "per class 1.0 1.0 0.5225 0.6666666666666666\n",
            "Test Balanced Accuracy 0.7972916666666666\n",
            "Test Accuracy 0.84583336\n",
            "per class 1.0 1.0 0.7166666666666667 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8458333333333333\n",
            "Test Accuracy 0.8102083\n",
            "per class 1.0 1.0 0.5741666666666667 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8102083333333333\n",
            "balanced acuracy 0.8181666666666667 +- 0.022826679383271382\n",
            "acuracy 0.8181666 +- 0.022826687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDQcZAZ4VGIA",
        "outputId": "20d4bc4a-3327-4112-8edc-fc2f7fe80edf"
      },
      "source": [
        "data_src_fft_ori = np.load(\"XsynallDEenv.npy\")\n",
        "label_src_ori = np.load(\"ysynallDEenv.npy\")\n",
        "data_tgt_fft_ori = np.load(\"XreallDEenv.npy\")\n",
        "label_tgt_ori = np.load(\"yreallDEenv.npy\")\n",
        "# group inner race/outer race/ball faults together\n",
        "label_src_ori = np.ceil(label_src_ori/3.).astype(np.int64) \n",
        "label_tgt_ori = np.ceil(label_tgt_ori/3.).astype(np.int64)\n",
        "\n",
        "# source data\n",
        "idx0 = np.where(label_src_ori==0)\n",
        "idx1 = np.where(label_src_ori==1)\n",
        "idx2 = np.where(label_src_ori==2)\n",
        "idx3 = np.where(label_src_ori==3)\n",
        "\n",
        "# avoid healthy sample overlap\n",
        "seed(0)\n",
        "idx_healthy = idx0[0].copy()\n",
        "np.random.shuffle(idx_healthy)\n",
        "num_healthy = len(idx_healthy)\n",
        "idx_src_healthy = np.concatenate([idx_healthy[:num_healthy//2]]*6, axis=0)\n",
        "idx_tgt_healthy = np.concatenate([idx_healthy[num_healthy//2:]]*6, axis=0)\n",
        "# balance source classes\n",
        "idx = np.concatenate([idx_src_healthy, idx1[0], idx2[0], idx3[0]], axis=0)\n",
        "label_src = label_src_ori[idx]\n",
        "data_src_fft = data_src_fft_ori[idx]\n",
        "\n",
        "# Target test data, avoid healthy sample overlap\n",
        "data_tgt_fft = np.concatenate([data_tgt_fft_ori[label_tgt_ori!=0], data_src_fft_ori[idx_tgt_healthy]], axis=0)\n",
        "label_tgt = np.concatenate([label_tgt_ori[label_tgt_ori!=0], label_src_ori[idx_tgt_healthy]], axis=0)\n",
        "\n",
        "\n",
        "# Target training data\n",
        "idx0 = np.where(label_tgt==0)\n",
        "idx1 = np.where(label_tgt==1)\n",
        "idx2 = np.where(label_tgt==2)\n",
        "idx3 = np.where(label_tgt==3)\n",
        "idx00 = idx0[0] # Healthy 1200 100%\n",
        "idx11 = np.random.choice(idx1[0], int(len(idx1[0]) * 0.05)) # Inner 10%\n",
        "idx22 = np.random.choice(idx2[0], int(len(idx2[0]) * 0.01)) # Rolling 1%\n",
        "idx33 = np.random.choice(idx3[0], int(len(idx3[0]) * 0.1)) # Outer 20%\n",
        "idx = np.concatenate([idx00 , idx11 , idx22 , idx33], axis=0)\n",
        "label_tgt_up = label_tgt[idx]\n",
        "data_tgt_fft_up = data_tgt_fft[idx]\n",
        "\n",
        "\n",
        "print(\"source\", \"0\", np.sum(label_src==0),\"1\",np.sum(label_src==1), \"2\",np.sum(label_src==2), \"3\", np.sum(label_src==3),)\n",
        "print(\"target train\", \"0\", np.sum(label_tgt_up==0),\"1\",np.sum(label_tgt_up==1), \"2\",np.sum(label_tgt_up==2), \"3\", np.sum(label_tgt_up==3),)\n",
        "print(\"target test\", \"0\", np.sum(label_tgt==0),\"1\",np.sum(label_tgt==1), \"2\",np.sum(label_tgt==2), \"3\", np.sum(label_tgt==3),)\n",
        "\n",
        "\n",
        "label_tgt_up = np.zeros_like(label_tgt_up) #mask out target labels\n",
        "\n",
        "class args:\n",
        "    batch_size=128\n",
        "    size=256\n",
        "    num_epoch=100\n",
        "    lr=0.001\n",
        "    model=\"dann\"\n",
        "    p=0.1\n",
        "\n",
        "\n",
        "res = []\n",
        "res_im = []\n",
        "for i in range(10): \n",
        "    args.seed=i    \n",
        "    acc_b, acc_i = run()\n",
        "    res.append(acc_b)\n",
        "    res_im.append(acc_i)\n",
        "print(\"balanced acuracy\", np.mean(res), \"+-\" , np.std(res))\n",
        "print(\"acuracy\", np.mean(res_im), \"+-\" , np.std(res_im))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source 0 1200 1 1200 2 1200 3 1200\n",
            "target train 0 1200 1 60 2 12 3 120\n",
            "target test 0 1200 1 1200 2 1200 3 1200\n",
            "Test Accuracy 0.72541666\n",
            "per class 0.995 1.0 0.17583333333333334 0.7308333333333333\n",
            "Test Balanced Accuracy 0.7254166666666667\n",
            "Test Accuracy 0.69354165\n",
            "per class 1.0 1.0 0.10166666666666667 0.6725\n",
            "Test Balanced Accuracy 0.6935416666666666\n",
            "Test Accuracy 0.76666665\n",
            "per class 1.0 1.0 0.315 0.7516666666666667\n",
            "Test Balanced Accuracy 0.7666666666666666\n",
            "Test Accuracy 0.70145833\n",
            "per class 0.995 1.0 0.07666666666666666 0.7341666666666666\n",
            "Test Balanced Accuracy 0.7014583333333333\n",
            "Test Accuracy 0.68104166\n",
            "per class 1.0 1.0 0.006666666666666667 0.7175\n",
            "Test Balanced Accuracy 0.6810416666666668\n",
            "Test Accuracy 0.6897917\n",
            "per class 1.0 1.0 0.030833333333333334 0.7283333333333334\n",
            "Test Balanced Accuracy 0.6897916666666667\n",
            "Test Accuracy 0.6983333\n",
            "per class 1.0 1.0 0.005833333333333334 0.7875\n",
            "Test Balanced Accuracy 0.6983333333333334\n",
            "Test Accuracy 0.7783333\n",
            "per class 1.0 1.0 0.44416666666666665 0.6691666666666667\n",
            "Test Balanced Accuracy 0.7783333333333333\n",
            "Test Accuracy 0.7675\n",
            "per class 1.0 1.0 0.3375 0.7325\n",
            "Test Balanced Accuracy 0.7675\n",
            "Test Accuracy 0.7547917\n",
            "per class 1.0 1.0 0.3408333333333333 0.6783333333333333\n",
            "Test Balanced Accuracy 0.7547916666666666\n",
            "balanced acuracy 0.7256875 +- 0.035632437089212196\n",
            "acuracy 0.7256875 +- 0.03563243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TahBzkx-m_61"
      },
      "source": [
        "## CDAN - Improved over DANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA3V8Os0nBSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a246df21-a60e-4b60-b5c8-4af95021857f"
      },
      "source": [
        "data_src_fft_ori = np.load(\"XsynallDEenv.npy\")\n",
        "label_src_ori = np.load(\"ysynallDEenv.npy\")\n",
        "data_tgt_fft_ori = np.load(\"XreallDEenv.npy\")\n",
        "label_tgt_ori = np.load(\"yreallDEenv.npy\")\n",
        "# group inner race/outer race/ball faults together\n",
        "label_src_ori = np.ceil(label_src_ori/3.).astype(np.int64) \n",
        "label_tgt_ori = np.ceil(label_tgt_ori/3.).astype(np.int64)\n",
        "\n",
        "# source data\n",
        "idx0 = np.where(label_src_ori==0)\n",
        "idx1 = np.where(label_src_ori==1)\n",
        "idx2 = np.where(label_src_ori==2)\n",
        "idx3 = np.where(label_src_ori==3)\n",
        "\n",
        "# avoid healthy sample overlap\n",
        "seed(0)\n",
        "idx_healthy = idx0[0].copy()\n",
        "np.random.shuffle(idx_healthy)\n",
        "num_healthy = len(idx_healthy)\n",
        "idx_src_healthy = np.concatenate([idx_healthy[:num_healthy//2]]*6, axis=0)\n",
        "idx_tgt_healthy = np.concatenate([idx_healthy[num_healthy//2:]]*6, axis=0)\n",
        "# balance source classes\n",
        "idx = np.concatenate([idx_src_healthy, idx1[0], idx2[0], idx3[0]], axis=0)\n",
        "label_src = label_src_ori[idx]\n",
        "data_src_fft = data_src_fft_ori[idx]\n",
        "\n",
        "# Target test data, avoid healthy sample overlap\n",
        "data_tgt_fft = np.concatenate([data_tgt_fft_ori[label_tgt_ori!=0], data_src_fft_ori[idx_tgt_healthy]], axis=0)\n",
        "label_tgt = np.concatenate([label_tgt_ori[label_tgt_ori!=0], label_src_ori[idx_tgt_healthy]], axis=0)\n",
        "\n",
        "\n",
        "# Target training data\n",
        "idx0 = np.where(label_tgt==0)\n",
        "idx1 = np.where(label_tgt==1)\n",
        "idx2 = np.where(label_tgt==2)\n",
        "idx3 = np.where(label_tgt==3)\n",
        "idx00 = idx0[0] # Healthy 1200 100%\n",
        "idx11 = np.random.choice(idx1[0], int(len(idx1[0]) * 0.05)) # Inner 10%\n",
        "idx22 = np.random.choice(idx2[0], int(len(idx2[0]) * 0.01)) # Rolling 1%\n",
        "idx33 = np.random.choice(idx3[0], int(len(idx3[0]) * 0.1)) # Outer 20%\n",
        "idx = np.concatenate([idx00 , idx11 , idx22 , idx33], axis=0)\n",
        "label_tgt_up = label_tgt[idx]\n",
        "data_tgt_fft_up = data_tgt_fft[idx]\n",
        "\n",
        "print(\"source\", \"0\", np.sum(label_src==0),\"1\",np.sum(label_src==1), \"2\",np.sum(label_src==2), \"3\", np.sum(label_src==3),)\n",
        "print(\"target train\", \"0\", np.sum(label_tgt_up==0),\"1\",np.sum(label_tgt_up==1), \"2\",np.sum(label_tgt_up==2), \"3\", np.sum(label_tgt_up==3),)\n",
        "print(\"target test\", \"0\", np.sum(label_tgt==0),\"1\",np.sum(label_tgt==1), \"2\",np.sum(label_tgt==2), \"3\", np.sum(label_tgt==3),)\n",
        "\n",
        "\n",
        "label_tgt_up = np.zeros_like(label_tgt_up) #mask out target labels\n",
        "\n",
        "class args:\n",
        "    batch_size=128\n",
        "    size=256\n",
        "    num_epoch=100\n",
        "    lr=0.001\n",
        "    model=\"cdan\"\n",
        "    p=0.1\n",
        "\n",
        "res = []\n",
        "res_im = []\n",
        "\n",
        "for i in range(10): \n",
        "    args.seed=i   \n",
        "    acc_b, acc_i = run()\n",
        "    res.append(acc_b)\n",
        "    res_im.append(acc_i)\n",
        "print(\"balanced acuracy\", np.mean(res), \"+-\" , np.std(res))\n",
        "print(\"acuracy\", np.mean(res_im), \"+-\" , np.std(res_im))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source 0 1200 1 1200 2 1200 3 1200\n",
            "target train 0 1200 1 60 2 12 3 120\n",
            "target test 0 1200 1 1200 2 1200 3 1200\n",
            "Test Accuracy 0.73270833\n",
            "per class 1.0 1.0 0.2625 0.6683333333333333\n",
            "Test Balanced Accuracy 0.7327083333333334\n",
            "Test Accuracy 0.69854164\n",
            "per class 1.0 1.0 0.1275 0.6666666666666666\n",
            "Test Balanced Accuracy 0.6985416666666666\n",
            "Test Accuracy 0.768125\n",
            "per class 1.0 1.0 0.3875 0.685\n",
            "Test Balanced Accuracy 0.7681250000000001\n",
            "Test Accuracy 0.75604165\n",
            "per class 0.995 1.0 0.3566666666666667 0.6725\n",
            "Test Balanced Accuracy 0.7560416666666666\n",
            "Test Accuracy 0.76104164\n",
            "per class 1.0 1.0 0.3775 0.6666666666666666\n",
            "Test Balanced Accuracy 0.7610416666666666\n",
            "Test Accuracy 0.763125\n",
            "per class 1.0 1.0 0.3775 0.675\n",
            "Test Balanced Accuracy 0.763125\n",
            "Test Accuracy 0.75125\n",
            "per class 1.0 1.0 0.3375 0.6675\n",
            "Test Balanced Accuracy 0.75125\n",
            "Test Accuracy 0.7825\n",
            "per class 1.0 1.0 0.44916666666666666 0.6808333333333333\n",
            "Test Balanced Accuracy 0.7825\n",
            "Test Accuracy 0.765625\n",
            "per class 1.0 1.0 0.3958333333333333 0.6666666666666666\n",
            "Test Balanced Accuracy 0.765625\n",
            "Test Accuracy 0.759375\n",
            "per class 1.0 1.0 0.37083333333333335 0.6666666666666666\n",
            "Test Balanced Accuracy 0.759375\n",
            "balanced acuracy 0.7538333333333334 +- 0.0220205869585713\n",
            "acuracy 0.75383335 +- 0.022020593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38QvkSCrkoQG"
      },
      "source": [
        "## Proposed conditional + Mixup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wenPijpckl2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855460bf-9801-44b0-9c74-fd260b223eed"
      },
      "source": [
        "data_src_fft_ori = np.load(\"XsynallDEenv.npy\")\n",
        "label_src_ori = np.load(\"ysynallDEenv.npy\")\n",
        "data_tgt_fft_ori = np.load(\"XreallDEenv.npy\")\n",
        "label_tgt_ori = np.load(\"yreallDEenv.npy\")\n",
        "# group inner race/outer race/ball faults together\n",
        "label_src_ori = np.ceil(label_src_ori/3.).astype(np.int64) \n",
        "label_tgt_ori = np.ceil(label_tgt_ori/3.).astype(np.int64)\n",
        "\n",
        "# source data\n",
        "idx0 = np.where(label_src_ori==0)\n",
        "idx1 = np.where(label_src_ori==1)\n",
        "idx2 = np.where(label_src_ori==2)\n",
        "idx3 = np.where(label_src_ori==3)\n",
        "\n",
        "# avoid healthy sample overlap\n",
        "seed(0)\n",
        "idx_healthy = idx0[0].copy()\n",
        "np.random.shuffle(idx_healthy)\n",
        "num_healthy = len(idx_healthy)\n",
        "idx_src_healthy = np.concatenate([idx_healthy[:num_healthy//2]]*6, axis=0)\n",
        "idx_tgt_healthy = np.concatenate([idx_healthy[num_healthy//2:]]*6, axis=0)\n",
        "# balance source classes\n",
        "idx = np.concatenate([idx_src_healthy, idx1[0], idx2[0], idx3[0]], axis=0)\n",
        "label_src = label_src_ori[idx]\n",
        "data_src_fft = data_src_fft_ori[idx]\n",
        "\n",
        "# Target test data, avoid healthy sample overlap\n",
        "data_tgt_fft = np.concatenate([data_tgt_fft_ori[label_tgt_ori!=0], data_src_fft_ori[idx_tgt_healthy]], axis=0)\n",
        "label_tgt = np.concatenate([label_tgt_ori[label_tgt_ori!=0], label_src_ori[idx_tgt_healthy]], axis=0)\n",
        "\n",
        "\n",
        "# Target training data\n",
        "idx0 = np.where(label_tgt==0)\n",
        "idx1 = np.where(label_tgt==1)\n",
        "idx2 = np.where(label_tgt==2)\n",
        "idx3 = np.where(label_tgt==3)\n",
        "idx00 = idx0[0] # Healthy 1200 100%\n",
        "idx11 = np.random.choice(idx1[0], int(len(idx1[0]) * 0.05)) # Inner 10%\n",
        "idx22 = np.random.choice(idx2[0], int(len(idx2[0]) * 0.01)) # Rolling 1%\n",
        "idx33 = np.random.choice(idx3[0], int(len(idx3[0]) * 0.1)) # Outer 20%\n",
        "idx = np.concatenate([idx00 , idx11 , idx22 , idx33], axis=0)\n",
        "label_tgt_up = label_tgt[idx]\n",
        "data_tgt_fft_up = data_tgt_fft[idx]\n",
        "\n",
        "print(\"source\", \"0\", np.sum(label_src==0),\"1\",np.sum(label_src==1), \"2\",np.sum(label_src==2), \"3\", np.sum(label_src==3),)\n",
        "print(\"target train\", \"0\", np.sum(label_tgt_up==0),\"1\",np.sum(label_tgt_up==1), \"2\",np.sum(label_tgt_up==2), \"3\", np.sum(label_tgt_up==3),)\n",
        "print(\"target test\", \"0\", np.sum(label_tgt==0),\"1\",np.sum(label_tgt==1), \"2\",np.sum(label_tgt==2), \"3\", np.sum(label_tgt==3),)\n",
        "\n",
        "label_tgt_up = np.zeros_like(label_tgt_up) #mask out target labels\n",
        "\n",
        "res = []\n",
        "res_im = []\n",
        "class args:\n",
        "    batch_size=128\n",
        "    size=256\n",
        "    num_epoch=100\n",
        "    lr=0.001\n",
        "    model=\"cdanmixup\"\n",
        "    p=0.1\n",
        "    beta=1.\n",
        "\n",
        "for i in range(10): \n",
        "    args.seed=i  \n",
        "    acc_b, acc_i = run()\n",
        "    res.append(acc_b)\n",
        "    res_im.append(acc_i)\n",
        "print(\"balanced acuracy\", np.mean(res), \"+-\" , np.std(res))\n",
        "print(\"acuracy\", np.mean(res_im), \"+-\" , np.std(res_im))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source 0 1200 1 1200 2 1200 3 1200\n",
            "target train 0 1200 1 60 2 12 3 120\n",
            "target test 0 1200 1 1200 2 1200 3 1200\n",
            "WARNING:tensorflow:From <ipython-input-3-466148e62d1e>:74: Beta.__init__ (from tensorflow.python.ops.distributions.beta) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/distributions/beta.py:208: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.82625\n",
            "per class 1.0 1.0 0.6383333333333333 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8262499999999999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.84166664\n",
            "per class 1.0 1.0 0.7 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8416666666666667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.81083333\n",
            "per class 1.0 1.0 0.5758333333333333 0.6675\n",
            "Test Balanced Accuracy 0.8108333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.84479165\n",
            "per class 1.0 1.0 0.7125 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8447916666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.82645833\n",
            "per class 1.0 1.0 0.6391666666666667 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8264583333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.7658333\n",
            "per class 1.0 1.0 0.395 0.6683333333333333\n",
            "Test Balanced Accuracy 0.7658333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.836875\n",
            "per class 1.0 1.0 0.6808333333333333 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8368749999999999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.825\n",
            "per class 1.0 1.0 0.6333333333333333 0.6666666666666666\n",
            "Test Balanced Accuracy 0.825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.81854165\n",
            "per class 1.0 1.0 0.6075 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8185416666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.83270836\n",
            "per class 1.0 1.0 0.6641666666666667 0.6666666666666666\n",
            "Test Balanced Accuracy 0.8327083333333333\n",
            "balanced acuracy 0.8228958333333333 +- 0.021383252549210852\n",
            "acuracy 0.8228958 +- 0.021383256\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}